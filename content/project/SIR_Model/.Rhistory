#   TPM[i, i - 1] = 1 - p #losing has probability 1-p
#   TPM[i, i + 1] = p     #winning has probability p
# }
# TPM[length(TPM)] = 1 #N -> N is an absorbing state
set.seed(1)
library("ggplot2")
#### Part A ####
#### Result: The gambler always loses!
#Defining Parameters
x0     = 3    #initial state
p      = 0.29 #probability of winning a round
N      = 10   #winning state
trials = 20   #number of radom realizations
for (i in 1:trials){
states   = x0   #vector to hold states, starting with the initial value
xn       = 0    #initializing the next stage
rounds   = 1    #number of rounds resets to 1
gambling = TRUE #boolean that is changed to false once an abosrbing state has been reached
while(gambling){
#Markov Chain: -1 (lose) w/ probability 1-p or +1 (win) w/ probability p
change = sample(c(-1,1), size=1, replace=TRUE, prob=c(1-p,p))
xn = states[rounds] + change  #computing next state based on previous
#Checking if absorbing state has been reached
if(xn == 0){
gambling = FALSE
}
if(xn == N){
gambling = FALSE
}
#Moving forward in time
rounds = rounds + 1
states = c(states, xn)
}
print(states) #output vector of results
}
##### Part B #####
#### Results:
####    The gambler always/almost always loses until p = 0.5
####    At p = 0.5, the probability of winning the game is slightly less than 0.5.
####    This is because the gambler starts closer to losing than he does to winning.
####    Above p = 0.5, the gambler always/almost always wins
#Defining Parameters
x0     = 4                       #initial state
p      = seq(0.1, 0.9, by = 0.1) #probability of winning a round
p_win  = rep(0, length(p))       #probability of winning the game for each p value
N      = 10                      #winning state
trials = 100                     #number of trials for each probability
for (k in 1:length(p)){
n_win      = 0 #number of times the gambler won
for (i in 1:trials){
states   = x0   #vector to hold states, starting with the initial value
xn       = 0    #initializing the next stage
rounds   = 1    #number of rounds resets to 1
gambling = TRUE #boolean that is changed to false once an abosrbing state has been reached
while(gambling){
#Markov Chain: -1 (lose) w/ probability 1-p or +1 (win) w/ probability p
change = sample(c(-1,1), size=1, replace=TRUE, prob=c(1-p[k],p[k]))
xn = states[rounds] + change  #computing next state based on previous
#Checking if absorbing state has been reached
if(xn == 0){
gambling = FALSE
}
if(xn == N){
gambling = FALSE
n_win = n_win + 1
}
#moving forward in time
rounds = rounds + 1
states = c(states, xn)
}
p_win[k] = n_win/trials #estimating probability of winning
}
}
ggplot(data = NULL, aes(x=p, y=p_win)) +
geom_point() +
xlab("Probability (p) of Winning a Round") +
ylab("Probability h(4,p) of Winning $10 Starting at $4") +
ggtitle("Gambler's Ruin Simulation with 100 Trials")
# #Computing Transition Probability Matrix (not used)
# TPM =  matrix(0,
#               nrow = N+1,
#               ncol = N+1,
#               byrow = TRUE)
# rownames(TPM) = c(0:N)
# colnames(TPM) = c(0:N)
#
# TPM[1] = 1 #0 -> 0 is an abosrbing state
# for (i in 2:(nrow(TPM) - 1)){
#   TPM[i, i - 1] = 1 - p #losing has probability 1-p
#   TPM[i, i + 1] = p     #winning has probability p
# }
# TPM[length(TPM)] = 1 #N -> N is an absorbing state
0.1*0.3
.9*.7
.3*.9
.1*.4
.1*.7
.9*.3
.1*.4
.1*.3
.9*.4
.9*.3
.1*.4
.1*.3
.1*..3
.1*.3
.9*.7
.9*.3
.1*.7
.9*.6
.1*.4
.1*.3
.9*.4
1.624+1.066
0.914+1.641
530/197
1510/591
x <- matrix(c(.6,.1,.3,.1,.7,.2,.2,.2,.6), nrow = 3, byrow = T)
x
y <- x
for(i in 1:25){y<- y %*% x}
y
HMC = function (U, grad_U, epsilon, L, current_q)
{
q = current_q
p = rnorm(length(q),0,1)  # independent standard normal variates
current_p = p
# Make a half step for momentum at the beginning
p = p - epsilon * grad_U(q) / 2
# Alternate full steps for position and momentum
for (i in 1:L)
{
# Make a full step for the position
q = q + epsilon * p
# Make a full step for the momentum, except at end of trajectory
if (i!=L) p = p - epsilon * grad_U(q)
}
# Make a half step for momentum at the end.
p = p - epsilon * grad_U(q) / 2
# Negate momentum at end of trajectory to make the proposal symmetric
p = -p
# Evaluate potential and kinetic energies at start and end of trajectory
current_U = U(current_q)
current_K = sum(current_p^2) / 2
proposed_U = U(q)
proposed_K = sum(p^2) / 2
# Accept or reject the state at end of trajectory, returning either
# the position at the end of the trajectory or the initial position
if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
{
return (q)  # accept
}
else {
return (current_q)  # reject
}
}
Ux = function(x){abs(x)}
grad_Ux = function(x){sign(x)}
pt_seq = (-1000:1000)/200
N_HMC = 20000
x0 = 1
x_old = x0
count_accept = 0
x_all = x_old
for(j in 1:N_HMC){
x_new = HMC(U = Ux,grad_U = grad_Ux,epsilon = 0.01, L = 20,current_q = x_old)
x_all = c(x_all, x_new)
if(x_new != x_old){count_accept = count_accept+1}
x_old = x_new
}
count_accept/N_HMC
plot(x_all)
hist(x_all, probability = T)
lines(x=pt_seq, y=exp(-Ux(pt_seq))/2)
plot(x=0:N_HMC, y=x_all, type="l")
l_seq = seq(from=-3,to=10, length.out=1001)
den_seq = 0.7*dnorm(l_seq) + 0.3*dnorm(l_seq,mean = 6)
px = function(x){
y= 0.7*dnorm(x) + 0.3*dnorm(x,mean = 6)
return(y)
} # density
plot(l_seq, den_seq, type="l")
Ux = function(x){
return(-log(px(x)))
} # potential energy
grad_Ux = function(x){
y= -0.7*x*dnorm(x) - 0.3*x*dnorm(x,mean = 6)
return(y/px(x))
# the derivative of the log p
}
set.seed(101)
N_HMC = 20000
x0 = 5
x_old = x0
count_accept = 0
x_all = x_old
for(j in 1:N_HMC){
x_new = HMC(U = Ux,grad_U = grad_Ux,epsilon = 0.005, L = 100,current_q = x_old)
x_all = c(x_all, x_new)
if(x_new != x_old){count_accept = count_accept+1}
x_old = x_new
}
count_accept/N_HMC
l_seq = seq(from=0,to=20, length.out=1001)
l_seq = seq(from=0,to=20, length.out=1001)
den_seq = dchisq(l_seq, df = 5)
plot(l_seq, den_seq, type="l")
set.seed(10)
mcmc_sigma = 0.5
N_mc = 30000
x0 = 2
x_mcmc = rep(NA, nrow=N_mc)
for(i in 1:N_mc){
x_proposal = rnorm(1,x0,mcmc_sigma)
p_acc = min(1, dchisq(x_proposal, 5)/(dchisq(x0, 5)))
U = rbinom(n=1,size=1,prob = p_acc)
if(U==1){
x0=x_proposal
}
x_mcmc[i] = x0
}
par(mar=c(2,4,2,1))
hist(x_mcmc, probability = T, col="lightgreen",xlim=c(0,14),
breaks=20, main="MCMC with 20000 points")
lines(l_seq, den_seq, lwd=2)
par(mar=c(2,4,2,1))
hist(x_mcmc[1:1000], probability = T, col="lightgreen",xlim=c(0,14),
breaks=20, main="MCMC: 1:1000")
lines(l_seq, den_seq, lwd=2)
par(mar=c(2,4,2,1))
plot(x_mcmc,type="l", xlab="x", main="MCMC: trajectory")
lines(l_seq, den_seq, lwd=2)
l_seq = seq(from=-3,to=10, length.out=1001)
den_seq = 0.7*dnorm(l_seq) + 0.3*dnorm(l_seq,mean = 6)
den_fn = function(x){
y= 0.7*dnorm(x) + 0.3*dnorm(x,mean = 6)
return(y)
}
plot(l_seq, den_seq, type="l")
set.seed(10)
mcmc_sigma = 1
N_mc = 30000
x0 = 6
x_mcmc = rep(NA, nrow=N_mc)
for(i in 1:N_mc){
x_proposal = rnorm(1,x0,mcmc_sigma)
p_acc = min(1, den_fn(x_proposal)/den_fn(x0))
U = rbinom(n=1,size=1,prob = p_acc)
if(U==1){
x0=x_proposal
}
x_mcmc[i] = x0
}
par(mar=c(2,4,2,1))
hist(x_mcmc, probability = T, col="lightgreen",xlim=c(-3,10),
breaks=20, main="MCMC with 30000 points, sigma=1")
lines(l_seq, den_seq, lwd=2)
par(mar=c(2,4,2,1))
plot(x_mcmc,type="l", xlab="x", main="MCMC: trajectory, sigma=1")
lines(l_seq, den_seq, lwd=2)
set.seed(10)
mcmc_sigma = 0.2
N_mc = 30000
x0 = 6
x_mcmc = rep(NA, nrow=N_mc)
for(i in 1:N_mc){
x_proposal = rnorm(1,x0,mcmc_sigma)
p_acc = min(1, den_fn(x_proposal)/den_fn(x0))
U = rbinom(n=1,size=1,prob = p_acc)
if(U==1){
x0=x_proposal
}
x_mcmc[i] = x0
}
par(mar=c(2,4,2,1))
hist(x_mcmc, probability = T, col="lightgreen",xlim=c(-3,10),
breaks=20, main="MCMC with 30000 points, sigma=0.2")
lines(l_seq, den_seq, lwd=2)
par(mar=c(2,4,2,1))
plot(x_mcmc,type="l", xlab="x", main="MCMC: trajectory, sigma=0.2")
lines(l_seq, den_seq, lwd=2)
set.seed(10)
mcmc_sigma = 1000
N_mc = 30000
x0 = 6
x_mcmc = rep(NA, nrow=N_mc)
for(i in 1:N_mc){
x_proposal = rnorm(1,x0,mcmc_sigma)
p_acc = min(1, den_fn(x_proposal)/den_fn(x0))
U = rbinom(n=1,size=1,prob = p_acc)
if(U==1){
x0=x_proposal
}
x_mcmc[i] = x0
}
par(mar=c(2,4,2,1))
hist(x_mcmc, probability = T, col="lightgreen",xlim=c(-3,10),
breaks=20, main="MCMC with 30000 points, sigma=1000")
lines(l_seq, den_seq, lwd=2)
par(mar=c(2,4,2,1))
plot(x_mcmc,type="l", xlab="x", main="MCMC: trajectory, sigma=1000")
lines(l_seq, den_seq, lwd=2)
set.seed(10)
library(ggplot2)
alpha_post <- function(a,b,t,exp.param = 1){
n = length(t) #number of observations
post = (b^a/gamma(a))^n*prod(t)^a*exp(-exp.param*a)
return(post)
}
N = 10000 #number of MCMC iterations
t = matrix(c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5),10)
y = matrix(c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22), 10)
alpha=matrix(0,N); alpha[1] = 1
alpha=matrix(0,N); alpha[1] = 100
beta=matrix(0,N); beta[1] = 100
theta=matrix(0,N,10,byrow=T); theta[1,] = 100
beta.param1 = 0.1
beta.param2 = 1
for(i in 2:N){
# 1) Simulate Candidate for Alpha
alpha[i]= rnorm(1,alpha[i-1],.5) #Candidate value
# 2) Evaluate Acceptance Probability
newpost = alpha_post(alpha[i],beta[i-1],theta[i-1,])   #value of posterior for candidate
oldpost = alpha_post(alpha[i-1],beta[i-1],theta[i-1,]) #value of posterior for previous value
p_acc = min(1, newpost/oldpost) #acceptance probability
# 3) Generate U
U = runif(1)
# 4) Choosing to Accept Candidate
# note that we assume the candidate was accepted, and change to previous if it was not
# two ways a candidate may be denied:
#   i) acceptance probability
#   ii) less than zero
if(U > p_acc){
alpha[i] = alpha[i-1]
}
if(alpha[i]<0){
alpha[i] = alpha[i-1]
}
# 5) Draw beta
beta[i]= rgamma(1, length(y)*alpha[i] + beta.param1, beta.param2 + sum(theta[i-1,]))
# 6) Draw theta
theta[i,] = rgamma(length(y),(alpha[i] + y),(beta[i] + t))
}
alpha=matrix(0,N); alpha[1] = 10
beta=matrix(0,N); beta[1] = 10
theta=matrix(0,N,10,byrow=T); theta[1,] = 10
beta.param1 = 0.1
beta.param2 = 1
for(i in 2:N){
# 1) Simulate Candidate for Alpha
alpha[i]= rnorm(1,alpha[i-1],.5) #Candidate value
# 2) Evaluate Acceptance Probability
newpost = alpha_post(alpha[i],beta[i-1],theta[i-1,])   #value of posterior for candidate
oldpost = alpha_post(alpha[i-1],beta[i-1],theta[i-1,]) #value of posterior for previous value
p_acc = min(1, newpost/oldpost) #acceptance probability
# 3) Generate U
U = runif(1)
# 4) Choosing to Accept Candidate
# note that we assume the candidate was accepted, and change to previous if it was not
# two ways a candidate may be denied:
#   i) acceptance probability
#   ii) less than zero
if(U > p_acc){
alpha[i] = alpha[i-1]
}
if(alpha[i]<0){
alpha[i] = alpha[i-1]
}
# 5) Draw beta
beta[i]= rgamma(1, length(y)*alpha[i] + beta.param1, beta.param2 + sum(theta[i-1,]))
# 6) Draw theta
theta[i,] = rgamma(length(y),(alpha[i] + y),(beta[i] + t))
}
ggplot(data=as.data.frame(alpha), aes(alpha)) +
geom_histogram() +
ylab("Frequency") +
xlab(expression("Sampled " ~alpha~ " values")) +
ggtitle("10,000 Samples from Conditonal Posterior")
ggplot(data=as.data.frame(beta), aes(beta)) +
geom_histogram() +
ylab("Frequency") +
xlab(expression("Sampled " ~beta~ " values")) +
ggtitle("10,000 Samples from Conditonal Posterior",
subtitle = expression(~beta~ " ~ Gam(10*" ~alpha~ " + 0.1, 1 + " ~Sigma~ ~theta[i]~ ")"))
theta = as.vector(theta)
set.seed(10)
library(ggplot2)
alpha_post <- function(a,b,t,exp.param = 1){
n = length(t) #number of observations
post = (b^a/gamma(a))^n*prod(t)^a*exp(-exp.param*a)
return(post)
}
N = 10000 #number of MCMC iterations
t = matrix(c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5),10)
y = matrix(c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22), 10)
alpha=matrix(0,N); alpha[1] = 1
beta=matrix(0,N); beta[1] = 1
theta=matrix(0,N,10,byrow=T); theta[1,] = 1
beta.param1 = 0.1
beta.param2 = 1
for(i in 2:N){
# 1) Simulate Candidate for Alpha
alpha[i]= rnorm(1,alpha[i-1],.5) #Candidate value
# 2) Evaluate Acceptance Probability
newpost = alpha_post(alpha[i],beta[i-1],theta[i-1,])   #value of posterior for candidate
oldpost = alpha_post(alpha[i-1],beta[i-1],theta[i-1,]) #value of posterior for previous value
p_acc = min(1, newpost/oldpost) #acceptance probability
# 3) Generate U
U = runif(1)
# 4) Choosing to Accept Candidate
# note that we assume the candidate was accepted, and change to previous if it was not
# two ways a candidate may be denied:
#   i) acceptance probability
#   ii) less than zero
if(U > p_acc){
alpha[i] = alpha[i-1]
}
if(alpha[i]<0){
alpha[i] = alpha[i-1]
}
# 5) Draw beta
beta[i]= rgamma(1, length(y)*alpha[i] + beta.param1, beta.param2 + sum(theta[i-1,]))
# 6) Draw theta
theta[i,] = rgamma(length(y),(alpha[i] + y),(beta[i] + t))
}
plot(alpha)
set.seed(10)
set.seed(10)
library(ggplot2)
alpha_post <- function(a,b,t,exp.param = 1){
n = length(t) #number of observations
post = (b^a/gamma(a))^n*prod(t)^a*exp(-exp.param*a)
return(post)
}
N = 10000 #number of MCMC iterations
t = matrix(c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5),10)
y = matrix(c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22), 10)
alpha=matrix(0,N); alpha[1] = 1
beta=matrix(0,N); beta[1] = 1
theta=matrix(0,N,10,byrow=T); theta[1,] = 1
beta.param1 = 0.1
beta.param2 = 1
for(i in 2:N){
# 1) Simulate Candidate for Alpha
alpha[i]= rnorm(1,alpha[i-1],.5) #Candidate value
# 2) Evaluate Acceptance Probability
newpost = alpha_post(alpha[i],beta[i-1],theta[i-1,])   #value of posterior for candidate
oldpost = alpha_post(alpha[i-1],beta[i-1],theta[i-1,]) #value of posterior for previous value
p_acc = min(1, newpost/oldpost) #acceptance probability
# 3) Generate U
U = runif(1)
# 4) Choosing to Accept Candidate
# note that we assume the candidate was accepted, and change to previous if it was not
# two ways a candidate may be denied:
#   i) acceptance probability
#   ii) less than zero
if(U > p_acc){
alpha[i] = alpha[i-1]
}
if(alpha[i]<0){
alpha[i] = alpha[i-1]
}
# 5) Draw beta
beta[i]= rgamma(1, length(y)*alpha[i] + beta.param1, beta.param2 + sum(theta[i-1,]))
# 6) Draw theta
theta[i,] = rgamma(length(y),(alpha[i] + y),(beta[i] + t))
}
ggplot(data=as.data.frame(alpha), aes(alpha)) +
geom_histogram() +
ylab("Frequency") +
xlab(expression("Sampled " ~alpha~ " values")) +
ggtitle("10,000 Samples from Conditonal Posterior")
library("dplyr")
library("FRK")
install.packages(FRK)
install.packages("FRK")
install.packages("IDE")
install.packages("sp")
install.packages("spacetime")
library("dplyr")
library("FRK")
library("ggplot2")
library("IDE")
library("sp")
library("spacetime")
SIM1 <- simIDE(T = 10, nobs = 100, k_spat_invariant = 1)
install.packages("latticeExtra")
library("blogdown", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
setwd("~/Desktop/website")
blogdown::serve_site()
shiny::runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
a <- "R_0 = "
b <- 2/3
paste(a, b)
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
runApp('ShinyApps/SIRModel')
library(rsconnect)
setwd("~/Desktop/website/ShinyApps")
rsconnect::deployApp('./SIRModel/')
setwd("~/Desktop/website/content/project/SIR_Model")
blogdown::serve_site()
blogdown::serve_site()
